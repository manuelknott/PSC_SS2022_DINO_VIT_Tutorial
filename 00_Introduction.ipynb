{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f56557f4-fc28-42a0-9f2f-87220971ac5d",
   "metadata": {},
   "source": [
    "# Facilitated machine learning for image-based fruit quality assessment\n",
    "\n",
    "This tutorial is based on the experiments described in [this preprint](https://arxiv.org/abs/2207.04523) of ours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2290697d-96f7-4bef-9b41-2b30b8c20603",
   "metadata": {},
   "source": [
    "## Vision Transformers\n",
    "\n",
    "- [Link to Paper](https://arxiv.org/abs/2010.11929)\n",
    "- Architecture adapter from Natural Language Processing\n",
    "- Key component is the \"self-attention\" mechanism\n",
    "\n",
    "\n",
    "\n",
    "![](assets/fig_vit.png \"Vision transformer concept visualized\")\n",
    "Model overview of a Vision Transformer. An input image is split into patches and embedded with\n",
    "positional information (“patch tokens”). An extra learnable cls-token is added. All tokens are transformed into a new sequence by a transformer encoder. In a classification setup, an additional simple sub-model (“classification head”) learns the the class based on the transformed cls-token.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88cb32e-2f80-4273-a683-f071b519a668",
   "metadata": {},
   "source": [
    "## Self-supervised learning with the DINO method\n",
    "\n",
    "- **S**elf **Di**stillation with **no** labels ([Link to paper](https://arxiv.org/abs/2104.14294))\n",
    "- Learning objective is not provided externally but rather bootstrapped from the data itself\n",
    "- Nice synergy with Vision Transformers for learning representations\n",
    "  - Attention maps are clearer focusing on foreground object compared to supervised approaches\n",
    "  - High Imagenet classification score with Linear Probing\n",
    "  \n",
    "![](assets/fig_dino_attention_maps.png \"Attention maps of vision transformers. Dino method vs supervised\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad161b6-df05-4c1e-8066-00ec9bb3b00d",
   "metadata": {},
   "source": [
    "## Facilitated ML with DINO ViTs\n",
    "\n",
    "- [Link to paper](https://arxiv.org/abs/2207.04523)\n",
    "- DINO ViTs can serve as \"off-the-shelf\" feature generators for unseen data sets without retraining or finetuning\n",
    "- Simplifies training procedure by enabling shallow machine learning\n",
    "- Can even be superior with small data sets\n",
    "\n",
    "\n",
    "![](assets/fig_highlevel_concept.png \"Highlevel Concept\")\n",
    "High-level comparison between the standard approach for machine learning–based image classi-\n",
    "fication and the method proposed in this work. We use a pre-trained Vision Transformer for the task of\n",
    "feature extraction. The generated CLS token can be seen as the equivalent of CNN embeddings. Since the\n",
    "training of the ViT and the classifier are decoupled we can use any shallow model for class prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156302cd-b391-4007-a58d-384ebfd6d596",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
