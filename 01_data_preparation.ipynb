{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29b4505-4fd4-4f1d-96f8-b83b77d99729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment, modify and run this of you work with Google colab to mount the required data\n",
    "\n",
    "#import os\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#os.chdir(\"/content/drive/MyDrive/DINO_VIT_Tutorial\") # insert your path where the downloaded data folder is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417d776d-1114-4a9c-a1ec-46585bfac6fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fruit quality related data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4633572a-0cf5-4b6b-a977-da6ee87e1024",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fayoum banana ripeness ([Mazen et al.](https://link.springer.com/article/10.1007/s13369-018-03695-5))\n",
    "\n",
    "![](assets/fig_fayoum_examples.png \"VExample images from the Fayoum banana dataset\")\n",
    "\n",
    "- 273 images\n",
    "- 4 ripeness classes\n",
    "\n",
    "Data set is published on [Google Drive](https://drive.google.com/drive/folders/1nRWBYAHNRqmL4R0SLrs6dbGQFSWGVY8V).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a33210-00f0-448f-b0f7-43c4652b836d",
   "metadata": {},
   "source": [
    "## THE CACS IFW database ([Purdue University](https://engineering.purdue.edu/RVL/Database/IFW/index.html))\n",
    "\n",
    "![](assets/fig_cascifw_examples.png \"Example images from the CASC IFW database\")\n",
    "\n",
    "- 5858 images\n",
    "- 2 classes: Healthy and damaged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72905a23-0c0c-43ff-88bf-2a1ac74c5a12",
   "metadata": {},
   "source": [
    "## Other data sets\n",
    "\n",
    "Feel free to use your own data set in this session or try out one of the following:\n",
    "\n",
    "- [Fruit Classification (Kaggle)](https://www.kaggle.com/datasets/sshikamaru/fruit-recognition)\n",
    "- [Fresh and Rotten Fruits (Kaggle)](https://www.kaggle.com/datasets/sriramr/fruits-fresh-and-rotten-for-classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1305c76-ef21-45d4-9a55-9d622408fe6e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pytorch Dataset Class\n",
    "\n",
    "You can easily create a custom dataset in pytorch:\n",
    "- Your custom class shoud inherit from the `torch.utils.data.Dataset` class-\n",
    "- You need to implement the `__len__` and `__getitem__` methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c191b0b-9ad6-4df2-aa69-b21fb3fd5b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # ignore warnigns\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d340e85-edd1-468d-8a88-50c854d39cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FolderDataset(Dataset):\n",
    "    \n",
    "    \"\"\"\n",
    "    Create a Pytorch dataset based on image folders.\n",
    "    Classes are expected to be subfolders of the specified directory.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, directory: str, resize=True):\n",
    "        if resize:\n",
    "            self.transform = transforms.Compose([\n",
    "                # step 1: if one dimension is bigger than the other we add padding pixels to the smaller one\n",
    "                lambda x: transforms.CenterCrop(max(x.shape[-2], x.shape[-1]))(x),\n",
    "                # step 2: downsample to desired resolution\n",
    "                transforms.Resize(224)])\n",
    "        else:\n",
    "            self.transform = None\n",
    "        \n",
    "        self.classes = [folder for folder in os.listdir(directory) if not folder.startswith(\".\")]\n",
    "        \n",
    "        self.all_files = []\n",
    "        for i, class_ in enumerate(self.classes):\n",
    "            for file in os.listdir(os.path.join(directory, class_)):\n",
    "                full_filepath = os.path.join(directory, class_, file)\n",
    "                if file.endswith(\".jpg\"):\n",
    "                    self.all_files.append((full_filepath, i))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.all_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a tuple of an image tensor and a class id)\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_path, label = self.all_files[idx]\n",
    "        image = torchvision.io.read_image(img_path) / 255\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b80eec-3315-4bb2-9ce5-d105aef56bf4",
   "metadata": {},
   "source": [
    "Let's test our class with the Banana dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1341ade8-4edf-48de-b370-fe9e3a466ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FolderDataset(\"./data/fayoum_banana/images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fe008e-7f3f-4535-acac-78e9cd3e2e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137c80cd-647e-42de-9ec5-4a240e7ea48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0f27b0-1fae-4e99-985f-486fc84d0407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import show_tensor_image\n",
    "show_tensor_image(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7acfc8-ddfe-4e04-b0d3-8ab84d4faae8",
   "metadata": {},
   "source": [
    "## Loading pretrained models\n",
    "\n",
    "Many pretrained models can loaded by just using the torch library. Those models are usually pretrained on the Imagenet data set at a resolution of 224 x 224 pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952720d2-afb9-4589-bf3a-75f896301a9f",
   "metadata": {},
   "source": [
    "### CNNs\n",
    "\n",
    "[Here](https://pytorch.org/vision/stable/models.html) you can find an overview of pretrained CNNs for classification.\n",
    "\n",
    "Pretrained CNNs from the torchvision library still have the dense classification layer ion the end. So we need to remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8194848d-e3ae-451f-99fd-a821d1cad74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = torchvision.models.resnet50(pretrained=True)\n",
    "resnet = torch.nn.Sequential(*list(resnet.children())[:-1]) # removing last layer of the model\n",
    "resnet.eval(); # inference mode, no gradients are tracked when passing input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9d2cbd-d5a2-422d-a468-9d7364c17794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify output shape\n",
    "image = torch.randn(4,3,224,224)\n",
    "print(f\"Input shape: {image.shape}\")\n",
    "output = resnet(image)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "output = output.squeeze()\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de63ed17-d960-4992-a3e3-3a6b0dfa6383",
   "metadata": {},
   "source": [
    "### DINO Vision Transformers\n",
    "\n",
    "You can choose between the following pretrained models.\n",
    "For more details, see [here](https://github.com/facebookresearch/dino).\n",
    "\n",
    "| arch          | params | patch size | token dims |\n",
    "|---------------|--------|------------|------------|\n",
    "| 'dino_vits16' | 21M    | 16         | 768        |\n",
    "| 'dino_vits8'  | 21M    | 8          | 384        |\n",
    "| 'dino_vitb16' | 85M    | 16         | 768        |\n",
    "| 'dino_vitb8'  | 85M    | 8          | 384        |\n",
    "\n",
    "Per default, these models return only the CLS token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5eefc1-8778-40b3-ab76-f9ebe98f3601",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = torch.hub.load('facebookresearch/dino:main', \"dino_vits16\")\n",
    "vit.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575af467-b030-4b9c-abbd-3c32ac6d8559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify output shape\n",
    "image = torch.randn(4,3,224,224)\n",
    "print(f\"Input shape: {image.shape}\")\n",
    "output = vit(image)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd729f6-38c4-4f36-a9f7-f2615da871e9",
   "metadata": {},
   "source": [
    "## Generate latent representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1223b2-69e2-4605-af71-31ff4f9d9e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "normalize_transform = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # normalize to imagenet mean/std\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_and_save_latents(model, model_name, data_loader, out_dir):\n",
    "    print(f\"Generating embeddings for {model_name}.\")\n",
    "    embeddings = []\n",
    "    for imgs, label in tqdm(loader):\n",
    "        imgs_norm = normalize_transform(imgs)\n",
    "        embeddings.append(model(imgs_norm).detach())\n",
    "    embeddings = torch.cat(embeddings).squeeze().detach().numpy()\n",
    "    np.save(os.path.join(out_dir, f\"{model_name}.npy\"), embeddings)\n",
    "\n",
    "generate_and_save_latents(resnet, \"resnet50\", loader, \"./data/fayoum_banana/embeddings\")\n",
    "generate_and_save_latents(vit, \"dino_vits16\", loader, \"./data/fayoum_banana/embeddings\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e398f6-af49-498c-aa3b-4b19b7b741b4",
   "metadata": {},
   "source": [
    "Now let's also save the corresponding class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c583cd6-6b36-4df5-b8f3-4e29409aa1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.cat([label for _, label in tqdm(loader)])\n",
    "np.save(f\"./data/fayoum_banana/embeddings/labels.npy\", labels.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba72782-6358-4e8d-85a5-b11ab40dbd0e",
   "metadata": {},
   "source": [
    "# Task (15 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86accf55-8761-49fd-9d5b-bbade0531755",
   "metadata": {},
   "source": [
    "- Setup this project in Google Colab or your local Jupyter Environment\n",
    "- Download the `data` folder as described in `README.md`\n",
    "- Verify that you can run this notebook without errors\n",
    "- generate embeddings for the Fayoum Banana dataset using a Neural Network of your choice (e.g. another CNN or another pretrained DINO ViT), and save them as a numpy file to the `./data/fayoum_banana/embeddings` directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf0ca54-d31d-4cc6-b51f-5fe3dc9daede",
   "metadata": {},
   "source": [
    "Note: As I am not allowed to share the original data set, embeddings for the apple data set are already generated for several models. You can find them in the `./data/cascifw_apple/embeddings` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edd727f-4c09-4d5f-bfac-266a817de58f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
